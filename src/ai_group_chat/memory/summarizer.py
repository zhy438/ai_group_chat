"""
æ™ºèƒ½æ‘˜è¦å™¨

ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œç»“æ„åŒ–æ‘˜è¦
"""

import asyncio
from typing import List, Optional
from loguru import logger

from ..models import Message
from ..llm.client import llm_client


# æ‘˜è¦æç¤ºè¯
SUMMARIZE_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€æ®µå¯¹è¯å†å²å‹ç¼©æˆç®€æ´çš„ç»“æ„åŒ–æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. ä¿ç•™å…³é”®ä¿¡æ¯ï¼šç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ã€é‡è¦å†³ç­–ã€ä»»åŠ¡çŠ¶æ€
2. åˆ é™¤å†—ä½™ï¼šç§»é™¤é‡å¤çš„è®¨è®ºè¿‡ç¨‹ã€æ— å…³çš„é—²èŠ
3. ä¿æŒç»“æ„ï¼šä½¿ç”¨æ¸…æ™°çš„åˆ†ç‚¹æ ¼å¼
4. æ§åˆ¶é•¿åº¦ï¼šæ‘˜è¦é•¿åº¦ä¸è¶…è¿‡åŸæ–‡çš„ 30%

è¾“å‡ºæ ¼å¼ï¼š
ğŸ“‹ å¯¹è¯æ‘˜è¦
- æ ¸å¿ƒè¯é¢˜ï¼š...
- å…³é”®ç»“è®ºï¼š...
- å¾…åŠäº‹é¡¹ï¼š...ï¼ˆå¦‚æœ‰ï¼‰
"""

SUMMARIZE_USER_PROMPT = """è¯·å¯¹ä»¥ä¸‹å¯¹è¯å†å²è¿›è¡Œæ‘˜è¦ï¼š

{conversation}

è¯·ç”Ÿæˆç®€æ´çš„ç»“æ„åŒ–æ‘˜è¦ï¼š"""


class Summarizer:
    """
    æ™ºèƒ½æ‘˜è¦å™¨
    
    ä½¿ç”¨ LLM å¯¹æ¶ˆæ¯è¿›è¡Œæ‘˜è¦
    """
    
    def __init__(self, model: str = "gpt-4o-mini"):
        """
        åˆå§‹åŒ–æ‘˜è¦å™¨
        
        Args:
            model: ç”¨äºæ‘˜è¦çš„æ¨¡å‹ID
        """
        self.model = model
        self.client = llm_client
    MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    RETRY_DELAY = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    
    async def summarize(self, messages: List[Message]) -> Optional[str]:
        """
        å¯¹æ¶ˆæ¯åˆ—è¡¨ç”Ÿæˆæ‘˜è¦
        
        åŒ…å«é‡è¯•é€»è¾‘ï¼šå¤±è´¥æ—¶æœ€å¤šé‡è¯•3æ¬¡
        
        Args:
            messages: éœ€è¦æ‘˜è¦çš„æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åˆ™è¿”å› None
        """
        if not messages:
            return None
        
        # æ„å»ºå¯¹è¯æ–‡æœ¬
        conversation_lines = []
        for msg in messages:
            sender = msg.sender_name or ("ç”¨æˆ·" if msg.role == "user" else "AI")
            conversation_lines.append(f"[{sender}]: {msg.content}")
        
        conversation_text = "\n".join(conversation_lines)
        user_prompt = SUMMARIZE_USER_PROMPT.format(conversation=conversation_text)
        
        # å¸¦é‡è¯•çš„ LLM è°ƒç”¨
        last_error = None
        for attempt in range(1, self.MAX_RETRIES + 1):
            try:
                summary = await self.client.chat(
                    model=self.model,
                    messages=[{"role": "user", "content": user_prompt}],
                    system_prompt=SUMMARIZE_SYSTEM_PROMPT,
                    temperature=0.3,
                    max_tokens=500,
                )
                
                logger.info(f"âœ… LLM æ‘˜è¦ç”ŸæˆæˆåŠŸï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰ï¼ŒåŸæ–‡ {len(conversation_text)} å­— â†’ æ‘˜è¦ {len(summary)} å­—")
                return summary
                
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ LLM æ‘˜è¦å¤±è´¥ï¼ˆç¬¬ {attempt}/{self.MAX_RETRIES} æ¬¡ï¼‰: {e}")
                
                if attempt < self.MAX_RETRIES:
                    await asyncio.sleep(self.RETRY_DELAY)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        logger.error(f"âŒ LLM æ‘˜è¦å½»åº•å¤±è´¥ï¼Œå·²é‡è¯• {self.MAX_RETRIES} æ¬¡: {last_error}")
        return None  # è¿”å› Noneï¼Œè®©ä¸Šå±‚å†³å®šä¸å‹ç¼©
    
    def summarize_sync(self, messages: List[Message]) -> Optional[str]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„æ‘˜è¦æ–¹æ³•ï¼ˆç”¨äºéå¼‚æ­¥ç¯å¢ƒï¼‰
        
        Returns:
            æ‘˜è¦æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.summarize(messages))
                    return future.result(timeout=60)  # å¢åŠ è¶…æ—¶ä»¥å®¹çº³é‡è¯•
            else:
                return loop.run_until_complete(self.summarize(messages))
        except Exception as e:
            logger.error(f"åŒæ­¥æ‘˜è¦å¤±è´¥: {e}")
            return None  # å¤±è´¥è¿”å› None


# å…¨å±€æ‘˜è¦å™¨å®ä¾‹
summarizer = Summarizer()
